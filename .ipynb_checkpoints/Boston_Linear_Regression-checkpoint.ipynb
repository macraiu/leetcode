{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "\n",
    "## SKLEARN LINEAR MODEL USES GRADIENT DESCENT AS OPTIMIZATION ALGORITHM\n",
    "\n",
    "$ w_j = w_j - \\alpha \\frac{1}{n} \\sum (Xw - y) x_j$\n",
    "\n",
    "### GRADIENT DESCENT IS SUBSCEPTIBLE TO SCALE: BUT THE ALGORITHM AUTOMATICALLY SCALES THE FEATURE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X, y = scale(boston.data), boston.target\n",
    "print(boston.data.shape, boston.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(X, y)\n",
    "print(regression.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE THE COEEFICIENT OF DETERMINATION\n",
    "## RANGE IN BETWEEN 0 AND 1\n",
    "$R^2 = 1 - \\frac{\\sum (Xw - y)^2}{\\sum (\\bar y - y)^2}$\n",
    "\n",
    "### SHOULD BE THE SAME AS REGRESSION SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(y)\n",
    "squared_errors_mean = np.sum((mean_y - y)**2)\n",
    "squared_errors_model = np.sum((y - regression.predict(X))**2)\n",
    "R2 = 1 - (squared_errors_model / squared_errors_mean)\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOD RESULT FOR LINEAR REGRESSION. VALUES OVER 0.90 ARE RARE ARE MIGHT BE DERIVATIVE FROM DATA SNOOPING OR LEAKAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM:-0.93', 'ZN:1.08', 'INDUS:0.14', 'CHAS:0.68', 'NOX:-2.06', 'RM:2.67', 'AGE:0.02', 'DIS:-3.1', 'RAD:2.66', 'TAX:-2.08', 'PTRATIO:-2.06', 'B:0.85', 'LSTAT:-3.74']\n"
     ]
    }
   ],
   "source": [
    "print([a + ':' + str(round(b, 2)) for a, b in zip(boston.feature_names, regression.coef_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIP FUNCTION CREATES TUPLE ARRAY BY COUPLING THE FIRST ELEMENT IN THE FIRST ARRAY QITH FIRST ELEMENT IN SECOND. THEN CONTINUES.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR QUALITATIVE MEASURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lbl = LabelEncoder()\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative = np.array(['red', 'red', 'green', 'blue', 'red', 'blue', 'blue', 'green']).reshape(8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qualitative = ['red', 'red', 'green', 'blue', 'red', 'blue', 'blue', 'green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = lbl.fit_transform(qualitative).reshape(8,1)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(type(enc.fit_transform(qualitative)))\n",
    "print(enc.fit_transform(qualitative).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF MISSING DATA THE MODEL MIGHT STOP\n",
    "### FIT MISSING DATA WITH 0 OR FIT WITH LINEAR INTERPOLATION OR WITH MEAN OF THAT FEATURE\n",
    "### OTHER SOLUTION: CREATE ANOTHER BINARY FEATURE WHICH SHOWS THAT A FEATURE IS MISSING.\n",
    "\n",
    "<br>\n",
    "\n",
    "### LINEAR REGRESSION IS DISRUPTED BY OUTLIERS. \n",
    "### THE MODEL TRIES TO MINIMISE THE SQUARE VALUE OF THE ERRORS (RESIDUALS).\n",
    "### THE OUTLIERS HAVE LARGE RESIDUAL!!\n",
    "\n",
    "<br>\n",
    "\n",
    "### LINEAR MODEL HAS ONE COEFFICIENT FOR EACH FEATURE. THIS IS PERFECT FOR EXPRESSING A UNCORRELATED FEATURE SITUATION. BUT AS AN EXAMPLE:  HUMAN AGE AND HAIR COLOR ARE CORRELATED.\n",
    "\n",
    "### USE POLYNOMIAL EXPANSION OF THE FEATURES where you consider a beta coefficient for the second degree of the features as follows:\n",
    "\n",
    "$y = b_1x_1 + b_2x_2 + a$\n",
    "\n",
    "to \n",
    "\n",
    "$ y = b_1x_1 + b_2x_2 + a + b_3 x_1^2 + b_4x_2^2 + b_5 x_1 x_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree = 2)\n",
    "poly_X = pf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n"
     ]
    }
   ],
   "source": [
    "print(poly_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for each feature couple combination created another 3 features with the polynomial expansion.\n",
    "### each one of these new features will have their own beta coefficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(poly_X, y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.824\n"
     ]
    }
   ],
   "source": [
    "# RIDGE DISTANCE\n",
    "reg_regression = Ridge(alpha = 0.2, normalize = True, max_iter = 1000)\n",
    "reg_regression.fit(X_train, y_train)\n",
    "\n",
    "print('R2: %0.3f' % r2_score(y_test, reg_regression.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what's happening?\n",
    "### split the polynomial dataset into train and test at 0.33 % for test.\n",
    "### use ridge distance\n",
    "### NORMALIZE DATA AS THE POLYNOMIAL EXPANSION FEATURES ARE SQUARED!!\n",
    "### train on train split data\n",
    "### find score of the regression predict of the X_test against the true value: y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USING POLYNOMIAL EXPANSION DECREASES BIAS "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
